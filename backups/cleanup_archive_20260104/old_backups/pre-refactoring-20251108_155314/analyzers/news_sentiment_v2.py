# news_sentiment_v2.py
"""
v2 ë²„ì „: AI ê¸°ë°˜ ê°ì„± ë¶„ì„ ë° ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš©

ì£¼ìš” ê¸°ëŠ¥:
- HuggingFace Transformersë¥¼ í™œìš©í•œ ê°ì„± ë¶„ì„ (ì„ íƒì )
- ì‹œê°„ ê°€ì¤‘ì¹˜: ìµœê·¼ ë‰´ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- ë¹„ë™ê¸° ì²˜ë¦¬ (async/await)
- ìºì‹œ ì‹œìŠ¤í…œ (JSON íŒŒì¼ ê¸°ë°˜)
- í•œêµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë„¤ì´ë²„ API)
"""
import asyncio
import aiohttp
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging
import re
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import os
from pathlib import Path

logger = logging.getLogger(__name__)

# HuggingFace transformers (ì„ íƒì )
try:
    from transformers import pipeline
    HF_AVAILABLE = True
except Exception:
    HF_AVAILABLE = False
    logger.info("HuggingFace transformers ë¯¸ì„¤ì¹˜ - í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„±ë¶„ì„ ì‚¬ìš©")


class NewsSentimentAnalyzerV2:
    """AI ê¸°ë°˜ ê°ì„± ë¶„ì„ê¸° (v2)

    Features:
      - HuggingFace sentiment model (ì„ íƒì , fallback: keyword method)
      - Time-weighted aggregation (ìµœê·¼ ë‰´ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜)
      - Async fetching (ë„¤ì´ë²„ ë‰´ìŠ¤ API)
      - Translation caching
      - DB fallback: JSON cache file
    """

    def __init__(self, symbols: List[str], hf_model_name: str = 'nlptown/bert-base-multilingual-uncased-sentiment',
                 max_concurrency: int = 6, client_id: str = None, client_secret: str = None):
        """
        ì´ˆê¸°í™”

        Args:
            symbols: ë¶„ì„í•  ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (í•œêµ­ ì£¼ì‹ ì½”ë“œ ë˜ëŠ” ì´ë¦„)
            hf_model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            max_concurrency: ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜
            client_id: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ID
            client_secret: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ì‹œí¬ë¦¿
        """
        self.symbols = symbols
        self.sentiment_scores = {s: 0.0 for s in symbols}
        self.last_update = None
        self._executor = ThreadPoolExecutor(max_workers=2)
        self._semaphore = asyncio.Semaphore(max_concurrency)

        # ë„¤ì´ë²„ API ì„¤ì •
        from dotenv import load_dotenv
        load_dotenv()
        self.client_id = client_id or os.getenv("NAVER_CLIENT_ID")
        self.client_secret = client_secret or os.getenv("NAVER_CLIENT_SECRET")

        # HF pipeline lazy init
        self.hf_model_name = hf_model_name
        self._hf_pipeline = None

        # caching
        self.translation_cache = {}  # {text_hash: translated_text}
        self.summary_cache = {}  # {symbol: {summary, updated_at}}
        cache_dir = Path('cache')
        cache_dir.mkdir(exist_ok=True)
        self.cache_file = cache_dir / 'news_sentiment_v2.json'

        # í‚¤ì›Œë“œ ì‚¬ì „ (í•œêµ­ì–´)
        self.positive_keywords = {
            'ê¸‰ë“±', 'ìƒìŠ¹', 'í˜¸ì¬', 'ì¦ê°€', 'ì‹¤ì ê°œì„ ', 'ìˆ˜ì£¼', 'ì‹ ì œí’ˆ', 'íŠ¹í—ˆ', 'íˆ¬ì', 'í™•ëŒ€',
            'ê¸ì •', 'ì„±ì¥', 'ë§¤ìˆ˜', 'ê°•ì„¸', 'ëŒíŒŒ', 'ì‹ ê³ ê°€', 'ìˆ˜ìµ', 'ì´ìµ', 'ê°œì„ ', 'í˜¸í™©'
        }
        self.negative_keywords = {
            'ê¸‰ë½', 'í•˜ë½', 'ì•…ì¬', 'ê°ì†Œ', 'ì‹¤ì ì•…í™”', 'ì·¨ì†Œ', 'ë¦¬ì½œ', 'ì†Œì†¡', 'ì ì', 'ê°ì›',
            'ë¶€ì •', 'ìœ„ì¶•', 'ë§¤ë„', 'ì•½ì„¸', 'í•˜íšŒ', 'ì‹ ì €ê°€', 'ì†ì‹¤', 'ì ì', 'ì•…í™”', 'ë¶ˆí™©'
        }

        # load caches
        self._load_disk_cache()

    def _load_disk_cache(self):
        """ë””ìŠ¤í¬ ìºì‹œ ë¡œë“œ"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.summary_cache = json.load(f)
                    logger.info(f"ìºì‹œ ë¡œë“œ: {len(self.summary_cache)}ê°œ í•­ëª©")
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _save_disk_cache(self):
        """ë””ìŠ¤í¬ ìºì‹œ ì €ì¥"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.summary_cache, f, ensure_ascii=False, indent=2, default=str)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _ensure_hf(self):
        """HuggingFace pipeline ì´ˆê¸°í™”"""
        if not HF_AVAILABLE:
            return None
        if self._hf_pipeline is None:
            try:
                self._hf_pipeline = pipeline('sentiment-analysis', model=self.hf_model_name)
                logger.info("HuggingFace ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"HF ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._hf_pipeline = None
        return self._hf_pipeline

    async def _hf_sentiment_async(self, texts: List[str]) -> List[float]:
        """HF ëª¨ë¸ì„ threadpoolì—ì„œ ì‹¤í–‰ (ë¹„ë™ê¸°)"""
        pipe = self._ensure_hf()
        if not pipe:
            # fallback to keyword
            return [self._keyword_sentiment(t) for t in texts]

        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(self._executor, pipe, texts)

        # ì ìˆ˜ ë§¤í•‘ (1-5 stars -> -1.0 to 1.0)
        mapped = []
        for r in results:
            label = r.get('label', '3 stars')
            # Extract number
            match = re.search(r'(\d+)', label)
            if match:
                stars = int(match.group(1))
                # 1 star = -1.0, 3 stars = 0.0, 5 stars = 1.0
                score = (stars - 3) / 2.0
                mapped.append(max(-1.0, min(1.0, score)))
            else:
                mapped.append(0.0)
        return mapped

    def _keyword_sentiment(self, text: str) -> float:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„"""
        if not text:
            return 0.0
        t = text.lower()
        pos = sum(1 for k in self.positive_keywords if k in t)
        neg = sum(1 for k in self.negative_keywords if k in t)
        total = pos + neg
        if total == 0:
            return 0.0
        return (pos - neg) / total

    def _time_weight(self, published: datetime) -> float:
        """ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚° (exponential decay)

        ìµœê·¼ ë‰´ìŠ¤ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ë†’ìŒ. 12ì‹œê°„ half-life.
        """
        age = (datetime.now() - published).total_seconds()
        half_life = 60 * 60 * 12  # 12ì‹œê°„
        weight = 2 ** (-age / half_life)
        return max(0.0, min(1.0, weight))

    async def _fetch_naver_news(self, query: str, display: int = 10) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (async)"""
        if not self.client_id or not self.client_secret:
            logger.warning("ë„¤ì´ë²„ API í‚¤ ë¯¸ì„¤ì • - ë‰´ìŠ¤ ìˆ˜ì§‘ ë¶ˆê°€")
            return []

        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        params = {
            "query": query,
            "display": display,
            "sort": "date"
        }

        async with self._semaphore:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, headers=headers, params=params, timeout=10) as resp:
                        if resp.status != 200:
                            logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ HTTP {resp.status} for {query}")
                            return []
                        data = await resp.json()
            except Exception as e:
                logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ fetch ì‹¤íŒ¨ for {query}: {e}")
                return []

        items = data.get('items', [])
        processed = []
        for item in items:
            title = self._remove_html_tags(item.get('title', ''))
            desc = self._remove_html_tags(item.get('description', ''))
            pub_date_str = item.get('pubDate', '')

            # Parse pub_date (RFC 822 format)
            pub_date = self._parse_rfc822_date(pub_date_str)

            processed.append({
                'title': title,
                'description': desc,
                'published': pub_date or datetime.now(),
                'link': item.get('link', '')
            })

        return processed

    def _remove_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        clean = re.compile('<.*?>')
        text = re.sub(clean, '', text)
        text = text.replace('&quot;', '"').replace('&amp;', '&')
        text = text.replace('&lt;', '<').replace('&gt;', '>')
        return text.strip()

    def _parse_rfc822_date(self, date_str: str) -> Optional[datetime]:
        """RFC 822 ë‚ ì§œ íŒŒì‹±"""
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except:
            return None

    async def analyze_symbol(self, symbol: str):
        """ì¢…ëª©ì— ëŒ€í•œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°ì„± ë¶„ì„"""
        news = await self._fetch_naver_news(symbol, display=10)

        if not news:
            logger.info(f"{symbol} ë‰´ìŠ¤ ì—†ìŒ")
            self.sentiment_scores[symbol] = 0.0
            return 0.0

        texts = []
        weights = []
        for item in news:
            txt = (item.get('title', '') + ' ' + item.get('description', '')).strip()
            if not txt:
                continue
            w = self._time_weight(item.get('published', datetime.now()))
            texts.append(txt)
            weights.append(w)

        if not texts:
            self.sentiment_scores[symbol] = 0.0
            return 0.0

        # HF ê°ì„± ë¶„ì„
        hf_scores = await self._hf_sentiment_async(texts)

        # ê°€ì¤‘ í‰ê· 
        total_w = sum(weights)
        if total_w == 0:
            weighted = 0.0
        else:
            weighted = sum(s * w for s, w in zip(hf_scores, weights)) / total_w

        # ìºì‹œ ì €ì¥
        now = datetime.now().isoformat()
        self.summary_cache[symbol] = {
            'sentiment_score': weighted,
            'news_count': len(news),
            'updated_at': now
        }
        self._save_disk_cache()

        self.sentiment_scores[symbol] = float(weighted)
        self.last_update = datetime.now()
        logger.info(f"{symbol} ê°ì„± ì ìˆ˜: {weighted:.2f} ({len(news)}ê±´)")
        return float(weighted)

    async def update_all(self):
        """ëª¨ë“  ì¢…ëª© ì—…ë°ì´íŠ¸"""
        logger.info("ğŸ“° ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ì‹œì‘...")
        tasks = [self.analyze_symbol(s) for s in self.symbols]
        await asyncio.gather(*tasks)
        logger.info("ğŸ“Š ê°ì„±ë¶„ì„ ì™„ë£Œ")

    def get_sentiment_score(self, symbol: str) -> float:
        """ì¢…ëª©ì˜ ê°ì„± ì ìˆ˜ ë°˜í™˜ (-1.0 ~ 1.0)"""
        return float(self.sentiment_scores.get(symbol, 0.0))

    def get_all_scores(self):
        """ëª¨ë“  ê°ì„± ì ìˆ˜ ë°˜í™˜"""
        return dict(self.sentiment_scores)


async def sentiment_updater_task(analyzer: NewsSentimentAnalyzerV2, interval_minutes: int = 15):
    """ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ íƒœìŠ¤í¬"""
    while True:
        try:
            await analyzer.update_all()
            await asyncio.sleep(interval_minutes * 60)
        except asyncio.CancelledError:
            logger.info("ê°ì„±ë¶„ì„ íƒœìŠ¤í¬ ì¢…ë£Œ")
            break
        except Exception as e:
            logger.error(f"ê°ì„±ë¶„ì„ íƒœìŠ¤í¬ ì˜¤ë¥˜: {e}")
            await asyncio.sleep(60)


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    import sys
    logging.basicConfig(level=logging.INFO)

    async def test():
        analyzer = NewsSentimentAnalyzerV2(symbols=['ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤'])
        await analyzer.update_all()

        for symbol in analyzer.symbols:
            score = analyzer.get_sentiment_score(symbol)
            print(f"{symbol}: {score:+.2f}")

    asyncio.run(test())
